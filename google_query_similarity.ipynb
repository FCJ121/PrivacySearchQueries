{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "import urllib\n",
    "import urllib2\n",
    "import unicodedata\n",
    "import string\n",
    "import nltk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"All lexical representations are to be used is the stemmed representation by the Porter stemmer\"\"\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Tokenizes and performs stemming on the tokens.\"\"\"\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Convert unicode to ascii, removes accents.\"\"\"\n",
    "def to_ascii(s):\n",
    "    return unicodedata.normalize('NFKD', s).encode('ascii', 'ignore')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Prepares for centroid calculation: \n",
    "- Calculate L2 norm for each row\n",
    "- Normalize the row by L2 norm\"\"\"\n",
    "def tfidf_normalize(row):\n",
    "    # Calculate L2 norm value.\n",
    "    l2norm = np.linalg.norm(row, 2)\n",
    "    \n",
    "    # Normalize row by L2 norm.\n",
    "    return row/l2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Takes in the sparse matrix\"\"\"\n",
    "def get_query_expansion_vector(vec):\n",
    "    # Calculate the vector.\n",
    "    normalized = np.apply_along_axis(tfidf_normalize, axis=1, arr=vec)\n",
    "    centroid = np.sum(normalized, axis=0)\n",
    "    qe = centroid/np.linalg.norm(centroid, 2)\n",
    "    return qe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Retrieves html result for a query pushed into the Google search engine.\"\"\"\n",
    "def get_query_html(query, limit):\n",
    "    address = \"http://www.google.com/search?q=%s&num=100&hl=en&start=0\" % (urllib.quote_plus(query))\n",
    "    request = urllib2.Request(address, None, {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11'})\n",
    "    urlfile = urllib2.urlopen(request)\n",
    "    page = urlfile.read()\n",
    "    \n",
    "    # Determine the amount of time needed to sleep\n",
    "    # before we yield control.\n",
    "    sleep_time = 3600/limit\n",
    "    sleep(randint(sleep_time, sleep_time+5))\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Retrives the tf-idf matrix for a query and candidate.\"\"\"\n",
    "def get_tfidf_matrices(es_q, es_c):\n",
    "    lq = len(es_q)\n",
    "    lc = len(es_c)\n",
    "    \n",
    "    # Combine the document sets for tf-idf calculation.\n",
    "    combined = es_q\n",
    "    combined.extend(es_c)\n",
    "    \n",
    "    # We want to get a new vectorizer for every string.\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "    tfs = tfidf.fit_transform(combined)\n",
    "    \n",
    "    vectors = tfs.toarray()\n",
    "    return vectors[0:lq-1, :], vectors[lq:lq+lc-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculates the kernel function value from two expansion sets rather\n",
    "than raw short string candidates, makes caching easier.\"\"\"\n",
    "def kval_es(es_q, es_c):\n",
    "    vq, vc = get_tfidf_matrices(es_q, es_c)\n",
    "    qe_q = get_query_expansion_vector(vq)\n",
    "    qe_c = get_query_expansion_vector(vc)\n",
    "    return np.inner(qe_q, qe_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Finds the value of the kernel function between \n",
    "query string q and candidate c, betther not use this,\n",
    "cause this is costly. Use the extended set version when\n",
    "handling bulk. Use this for checking or debugging.\"\"\"\n",
    "def kval(q, c):\n",
    "    q_page = get_query_html(q, 1200)\n",
    "    c_page = get_query_html(c, 1200)\n",
    "    es_q = google_expanded_docs(q_page)\n",
    "    es_c = google_expanded_docs(c_page)\n",
    "    return kval_es(es_q, es_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Retrieves the related searches for a Google query string,\n",
    "given the html page of the google search page.\"\"\"\n",
    "def google_related_searches(page):    \n",
    "    rs = list()        \n",
    "    if page is not None:\n",
    "        soup = BeautifulSoup(page, 'lxml')        \n",
    "        # Strip the related search portion using beautifulsoup.    \n",
    "        rsdiv = soup.find(\"div\", { \"id\" : \"brs\" })\n",
    "        for d in rsdiv.findAll('div', {'class':'brs_col'}):\n",
    "            for p in d.findAll('p', {'class':'_e4b'}):\n",
    "                rs.append(to_ascii(p.getText()).translate(None, string.punctuation))\n",
    "        \n",
    "    return rs\n",
    "\n",
    "\"\"\"Retrieves the first 100 expanded sets for a query string,\n",
    "given the html page of the google search page.Expanded set \n",
    "consists of the result header and the summary text\n",
    "strip for each of the search results in the returned page.\"\"\"\n",
    "def google_expanded_docs(page):\n",
    "    es = list()\n",
    "    if page is not None:\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        # Strip the extended set using beautifulsoup.\n",
    "        esdivs = soup.findAll(\"div\", { \"class\" : \"g\" })    \n",
    "        for esdiv in esdivs:\n",
    "            for d in esdiv.findAll('div', {'class':'rc'}):\n",
    "                doc = \"\"\n",
    "                for hr in d.findAll('h3', {'class':'r'}):\n",
    "                    doc += hr.getText()\n",
    "                    doc += ' '\n",
    "                for ds in d.findAll('div', {'class':'s'}):\n",
    "                    for s in ds.findAll('span', {'class' : 'st'}):\n",
    "                        doc += s.getText()                    \n",
    "                es.append(to_ascii(doc).translate(None, string.punctuation))\n",
    "            \n",
    "    return es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_kval(q, c):\n",
    "    k = kval(q, c)\n",
    "    print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.235908682104\n",
      "CPU times: user 769 ms, sys: 43.5 ms, total: 812 ms\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = 'delete cookies'\n",
    "c = 'cookies recipe'    \n",
    "test_kval(q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.214376190264\n",
      "CPU times: user 881 ms, sys: 88.5 ms, total: 969 ms\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = 'delete cookies'\n",
    "c = 'chocolate cookies'    \n",
    "test_kval(q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787332940935\n",
      "CPU times: user 731 ms, sys: 27.9 ms, total: 758 ms\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = 'delete cookies'\n",
    "c = 'how to delete cookies on chrome'    \n",
    "test_kval(q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rel_queries(l):\n",
    "    q = list()\n",
    "    for s in l:\n",
    "        r = google_related_searches(s)\n",
    "        q.extend(r)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = 'anonymous web proxy'\n",
    "first = google_related_searches(q)\n",
    "second = get_rel_queries(first)\n",
    "third = get_rel_queries(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: anonymous web proxy\n",
      "1st iteration\n",
      "=============\n",
      "unblocked proxy sites              0.625885275915\n",
      "proxy meaning                      0.341629813043\n",
      "best proxy server                  0.543056710652\n",
      "proxy sites list                   0.571541581749\n",
      "proxy sites for youtube            0.580685721987\n",
      "free proxy list                    0.49021895171\n",
      "skull proxy                        0.445939834306\n",
      "proxy sites for school             0.546793161541\n"
     ]
    }
   ],
   "source": [
    "print 'Seed: ' + q\n",
    "print '1st iteration'\n",
    "print '============='\n",
    "for c in first:\n",
    "    print c.ljust(35) + str(kval(q, c))\n",
    "    sleep(randint(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: anonymous web proxy\n",
      "2nd iteration\n",
      "=============\n",
      "unblocked proxy sites for school   0.520873715037\n",
      "unblocked proxy sites 2016         0.552267777783\n",
      "new proxy sites                    0.613555317019\n",
      "unblock blocked websites           0.30676562345\n",
      "unblocked proxies for school 2016  0.46395945235\n",
      "unblocked proxy list               0.639060012854\n",
      "fresh unblocked proxy sites        0.605553770321\n",
      "unblock websites                   0.41211288729\n",
      "proxy war meaning                  0.162943518505\n",
      "proxy meaning in hindi             0.25767848335\n",
      "proxy synonym                      0.241219822191\n",
      "proxy meaning in telugu            0.218214585157\n",
      "proxy meaning in tamil             0.234457407832\n",
      "proxy in a sentence                0.20749551871\n",
      "proxy definition creepypasta       0.196357555054\n",
      "proxy definition science           0.245467175434\n",
      "skull proxy                        0.449638036921\n",
      "best proxy server software         0.422672614541\n",
      "skullproxy                         0.438836239905\n",
      "top proxy sites                    0.562128384019\n",
      "best proxy sites                   0.599453327681\n",
      "proxify                            0.43194014335\n",
      "top 5 proxy sites                  0.512270793972\n",
      "england proxy                      0.467744824414\n",
      "best proxy server list             0.485292243694\n",
      "skull proxy                        0.447411223736\n",
      "best proxy server software         0.422672614541\n",
      "proxy site list 2016               0.433487141584\n",
      "proxify                            0.43194014335\n",
      "england proxy                      0.469751302233\n",
      "skullproxy                         0.436227154974\n",
      "best proxy sites for videos        0.489906107029\n",
      "youtube proxy free                 0.61799680137\n",
      "youtube proxy list                 0.607709887831\n",
      "proxy sites list                   0.54022782548\n",
      "unblock youtube videos             0.28540510515\n",
      "how to unblock youtube without proxy0.277191039079\n",
      "unblock youtube at school          0.278755818276\n",
      "proxy sites for facebook           0.521453112833\n",
      "unblock proxy                      0.646903696455\n",
      "free proxy list usa                0.506750903841\n",
      "Error retrieving google search results: HTTP Error 503: Service Unavailable\n",
      "Google throttling, wait a couple of minutes and try again.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "print 'Seed: ' + q\n",
    "print '2nd iteration'\n",
    "print '============='\n",
    "for c in second:\n",
    "    print c.ljust(35) + str(kval(q, c))\n",
    "    sleep(randint(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: anonymous web proxy\n",
      "3rd iteration\n",
      "=============\n",
      "Error retrieving google search results: HTTP Error 503: Service Unavailable\n",
      "Error retrieving google search results: HTTP Error 503: Service Unavailable\n",
      "Google throttling, wait a couple of minutes and try again.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "print 'Seed: ' + q\n",
    "print '3rd iteration'\n",
    "print '============='\n",
    "for c in third:\n",
    "    print c.ljust(35) + str(kval(q, c))\n",
    "    sleep(randint(3, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
